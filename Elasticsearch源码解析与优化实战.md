# Elasticsearch

## 基本概念原理

Elasticsearch是实时的分布式搜索分析引擎，内部使用Lucene做索引与搜索。

分布式意味着可以动态调整集群规模，弹性扩容，而这一切操作起来都非常简便，用户甚至不必了解集群原理就可以实现。

数据分片以提高水平扩展能力，分布式存储中还会把数据复制成多个副本，放置到不同的机器中，这样一来可以增加系统可用性，同时数据副本还可以使读操作并发执行，分担集群压力。但是多数据副本也带来了一致性的问题：部分副本写成功，部分副本写失败。

ES将数据副本分为主从两部分，即主分片（primaryshard）和副分片（replica shard）。主数据作为权威数据，写过程中先写主分片，成功后再写副分片，恢复阶段以主分片为准。

分片（shard）是底层的基本读写单元，分片的目的是分割巨大索引，让读写可以并行操作，由多台机器共同完成。读写请求最终落到某个分片上，分片可以独立执行读写工作。ES利用分片将数据分发到集群内各处。分片是数据的容器，文档保存在分片内，不会跨分片存储。分片又被分配到集群内的各个节点里。当集群规模扩大或缩小时，ES 会自动在各节点中迁移分片，使数据仍然均匀分布在集群里。

一个ES索引包含很多分片，一个分片是一个Lucene的索引，它本身就是一个完整的搜索引擎，可以独立执行建立索引和搜索任务。Lucene索引又由很多分段组成，每个分段都是一个倒排索引。ES每次“refresh”都会生成一个新的分段，其中包含若干文档的数据。在每个分段内部，文档的不同字段被单独建立索引。

索引建立的时候就需要确定好主分片数 副分片数可以随时修改

搜索1个有着50个分片的索引与搜索50个每个都有1个分片的索引完全等价

以_id 为单位删除文档不会立刻释放空间，删除的 doc 只在 Lucene分段合并时才会真正从磁盘中删除。即使手工触发分段合并，仍然会引起较高的 I/O 压力，并且可能因为分段巨大导致在合并过程中磁盘空间不足（分段大小大于磁盘可用空间的一半）。因此，我们建议周期性地创建新索引。

集群整体分片数量较多，集群管理的总分片数越多压力就越大。可以使用_shrink API来缩减主分片数量，降低集群负载。

倒排索引一旦被写入文件后就具有不变性，不变性具有许多好处：对文件的访问不需要加锁，读取索引时可以被文件系统缓存等。

新增内容并写到一个新的倒排索引中，查询时，每个倒排索引都被轮流查询，查询完再对结果进行合并。每次内存缓冲的数据被写入文件时，会产生一个新的Lucene段，每个段都是一个倒排索引。在一个记录元信息的文件中描述了当前Lucene索引都含有哪些分段。由于分段的不变性，更新、删除等操作实际上是将数据标记为删除，记录到单独的位置，这种方式称为标记删除。因此删除部分数据不会释放磁盘空间。

近实时搜索

一般情况下（direct方式除外），通过操作系统write接口写到磁盘的数据先到达系统缓存（内存）,write函数返回成功时，数据未必被刷到磁盘。通过手工调用flush，或者操作系统通过一定策略将系统缓存刷到磁盘。这种策略大幅提升了写入效率。从write函数返回成功开始，无论数据有没有被刷到磁盘，该数据已经对读取可见。

每秒产生一个新分段，新段先写入文件系统缓存，但稍后再执行flush刷盘操作，写操作很快会执行完，一旦写成功，就可以像其他文件一样被打开和读取了。

通用的做法是记录事务日志，每次对ES进行操作时均记录事务日志，当ES启动的时候，重放translog中所有在最后一次提交后发生的变更操作。

在ES中，每秒清空一次写缓冲，将这些数据写入文件，这个过程称为refresh，每次refresh会创建一个新的Lucene 段。但是分段数量太多会带来较大的麻烦，每个段都会消耗文件句柄、内存。

在ES中，每秒清空一次写缓冲，将这些数据写入文件，这个过程称为refresh，每次refresh会创建一个新的Lucene 段。但是分段数量太多会带来较大的麻烦，每个段都会消耗文件句柄、内存。

段越多，搜索也就越慢 常用的方案是选择大小相似的分段进行合并。在合并过程中，标记为删除的数据不会写入新分段，当合并过程结束，旧的分段数据被删除，标记删除的数据才从磁盘删除。

如果段文件设置一定上限不再合并，则对表中部分数据无法实现真正的物理删除。ES存在同样的问题。

es的refresh是lucene的flush，es的flush是lucene的commit

主从模式可以简化系统设计，Master作为权威节点，部分操作仅由Master执行，并负责维护集群元信息。缺点是Master节点存在单点故障，需要解决灾备问题，并且集群规模会受限于Master节点的管理能力。

主节点 数据节点 预处理节点 协调节点

集群健康状态分为三种：· Green，所有的主分片和副分片都正常运行。· Yellow，所有的主分片都正常运行，但不是所有的副分片都正常运行。这意味着存在单点故障风险。· Red，有主分片没能正常运行。每个索引也有上述三种状态

集群状态元数据是全局信息，元数据包括内容路由信息、配置信息等，其中最重要的是内容路由信息，它描述了“哪个分片位于哪个节点”这种信息。

集群状态由主节点负责维护，如果主节点从数据节点接收更新，则将这些更新广播到集群的其他节点，让每个节点上的集群状态保持最新。

分片分配过程中除了让节点间均匀存储，还要保证不把主分片和副分片分配到同一节点，避免单个节点故障引起数据丢失。

ES中几个基础模块的功能

Cluster模块是主节点执行集群管理的封装实现，管理集群状态，维护集群层面的配置信息。

allocation封装了分片分配相关的功能和策略，包括主分片的分配和副分片的分配，本模块由主节点调用。创建新索引、集群完全重启都需要分片分配的过程。

Discovery发现模块负责发现集群中的节点，以及选举主节点。当节点加入或退出集群时，主节点会采取相应的行动。zookeeper

gateway负责对收到Master广播下来的集群状态（cluster state）数据的持久化存储，并在集群完全重启时恢复它们。

Transport传输模块用于集群内节点之间的内部通信。从一个节点到另一个节点的每个请求都使用传输模块。异步

Engine模块封装了对Lucene的操作及translog的调用，它是对一个分片读写操作的最终提供者

## 集群启动流程

经历选举主节点、主分片、数据恢复等重要阶段

ES的选主算法是基于Bully算法的改进，主要思路是对节点ID排序，取ID值最大的节点作为Master

三个附加约定条件

- 参选人数需要过半，达到 quorum（多数）后就选出了临时的主
- 得票数需过半。某节点被选为主节点，必须判断加入它的节点数过半，才确认Master身份。
- 当探测到节点离开事件时，必须判断当前节点数是否过半。如果达不到quorum，则放弃Master身份，重新加入集群。 防止脑裂

discovery.zen.minimum_master_nodes

被选出的 Master 和集群元信息的新旧程度没有关系。因此它的第一个任务是选举元信息，让各节点把各自存储的元信息发过来，根据版本号确定最新的元信息，然后把这个信息广播下去，这样集群的所有节点都有了最新的元信息

集群元信息的选举包括两个级别：集群级和索引级。不包含哪个shard存于哪个节点这种信息。这种信息以节点磁盘存储的为准，需要上报。因为读写流程是不经过Master的，Master 不知道各 shard 副本直接的数据差异。

allocation过程

选举shard级元信息，构建内容路由表，是在allocation模块完成的。在初始阶段，所有的shard都处于UNASSIGNED（未分配）状态。ES中通过分配过程决定哪个分片位于哪个节点，重构内容路由表。此时，首先要做的是分配主分片。

所有的分配工作都是 Master 来做的，此时， Master不知道主分片在哪，它向集群的所有节点询问：大家把[website][0]分片的元信息发给我。然后，Master 等待所有的请求返回，正常情况下它就有了这个 shard 的信息，然后根据某种策略选一个分片作为主分片。是不是效率有些低？这种询问量=shard 数×节点数。所以说我们最好控制shard的总规模别太大。

从ES 5.x开始，主分片选举过程是通过集群级元信息中记录的“最新主分片的列表”来确定主分片的：汇报信息中存在，并且这个列表中也存在。

如果集群设置了：＂cluster.routing.allocation.enable＂: ＂none＂禁止分配分片，集群仍会强制分配主分片。因此，在设置了上述选项的情况下，集群重启后的状态为Yellow，而非Red。

allocation过程中允许新启动的节点加入集群。

index recovery

分片分配成功后进入recovery流程。主分片的recovery不会等待其副分片分配成功才开始recovery。它们是独立的流程，只是副分片的recovery需要主分片恢复完毕才开始。

为什么需要recovery？对于主分片来说，可能有一些数据没来得及刷盘；对于副分片来说，一是没刷盘，二是主分片写完了，副分片还没来得及写，主副分片数据不一致。

1． 主分片recovery由于每次写操作都会记录事务日志（translog），事务日志中记录了哪种操作，以及相关的数据。因此将最后一次提交（Lucene 的一次提交就是一次 fsync 刷盘的过程）之后的 translog中进行重放，建立Lucene索引，如此完成主分片的recovery。

2． 副分片recovery副分片的恢复是比较复杂的，在ES的版本迭代中，副分片恢复策略有过不少调整。副分片需要恢复成与主分片一致，同时，恢复期间允许新的索引操作。在目前的6.0版本中，恢复分成两阶段执行。· phase1：在主分片所在节点，获取translog保留锁，从获取保留锁开始，会保留translog不受其刷盘清空的影响。然后调用Lucene接口把shard做快照，这是已经刷磁盘中的分片数据。把这些shard数据复制到副本节点。在phase1完毕前，会向副分片节点发送告知对方启动engine，在phase2开始之前，副分片就可以正常处理写请求了。· phase2：对translog做快照，这个快照里包含从phase1开始，到执行translog快照期间的新增索引。将这些translog发送到副分片所在节点进行重放。

由于需要支持恢复期间的新增写操作（让ES的可用性更强），这两个阶段中需要重点关注以下几个问题。

- 分片数据完整性：如何做到副分片不丢数据？第二阶段的 translog 快照包括第一阶段所有的新增操作。那么第一阶段执行期间如果发生“Lucene commit”（将文件系统写缓冲中的数据刷盘，并清空translog），清除translog怎么办？在ES2.0之前，是阻止了刷新操作，以此让translog都保留下来。从2.0版本开始，为了避免这种做法产生过大的translog，引入了translog.view的概念，创建 view 可以获取后续的所有操作。从6.0版本开始，translog.view 被移除。引入TranslogDeletionPolicy的概念，它将translog做一个快照来保持translog不被清理。这样实现了在第一阶段允许Lucene commit。
- 数据一致性：在ES 2.0之前，副分片恢复过程有三个阶段，第三阶段会阻塞新的索引操作，传输第二阶段执行期间新增的translog，这个时间很短。自2.0版本之后，第三阶段被删除，恢复期间没有任何写阻塞过程。在副分片节点，重放translog时，phase1和phase2之间的写操作与phase2重放操作之间的时序错误和冲突，通过写流程中进行异常处理，对比版本号来过滤掉过期操作。这样，时序上存在错误的操作被忽略，对于特定的 doc，只有最新一次操作生效，保证了主副分片一致。

第一阶段尤其漫长，因为它需要从主分片拉取全量的数据。在ES 6.x中，对第一阶段再次优化：标记每个操作。在正常的写操作中，每次写入成功的操作都分配一个序号，通过对比序号就可以计算出差异范围，在实现方式上，添加了globalcheckpoint和local checkpoint，主分片负责维护global checkpoint，代表所有分片都已写入这个序号的位置，local checkpoint代表当前分片已写入成功的最新位置，恢复时通过对比两个序列号，计算出缺失的数据范围，然后通过translog重放这部分数据，同时translog会为此保留更长的时间。因此，有两个机会可以跳过副分片恢复的phase1：基于SequenceNumber，从主分片节点的translog恢复数据；主副两分片有相同的syncid且doc数相同，可以跳过phase1。

当一个索引的主分片分配成功后，到此分片的写操作就是允许的。当一个索引所有的主分片都分配成功后，该索引变为Yellow。当全部索引的主分片都分配成功后，整个集群变为Yellow。当一个索引全部分片分配成功后，该索引变为 Green。当全部索引的索引分片分配成功后，整个集群变为Green。

索引数据恢复是最漫长的过程。当shard总量达到十万级的时候，6.x之前的版本集群从Red变为Green的时间可能需要小时级。ES 6.x中的副本允许从本地translog恢复是一次重大的改进，避免了从主分片所在节点拉取全量数据，为恢复过程节约了大量时间。

## 节点启动和关闭

检测外部环境 堆大小、文件描述符、内存锁定、最大线程数等

内部模块启动，启动子模块，初始化内部数据、创建线程池、启动线程池

启动keepalive线程 调用keepAliveThread.start（）方法启动keepalive线程，线程本身不做具体的工作。主线程执行完启动流程后会退出，keepalive线程是唯一的用户线程，作用是保持进程运行。在Java程序中，至少要有一个用户线程。当用户线程数为零时退出进程。

ES进程会捕获SIGTERM信号（kill命令默认信号）进行处理，调用各模块的stop方法，让它们有机会停止服务，安全退出。进程重启期间，如果主节点被关闭，则集群会重新选主，在这期间，集群有一个短暂的无主状态。如果集群中的主节点是单独部署的，则新主当选后，可以跳过gateway和recovery流程，否则新主需要重新分配旧主所持有的分片：提升其他副本为主分片，以及分配新的副分片。如果数据节点被关闭，则读写请求的TCP连接也会因此关闭，对客户端来说写操作执行失败。但写流程已经到达Engine环节的会正常写完，只是客户端无法感知结果。此时客户端重试，如果使用自动生成ID，则数据内容会重复。

滚动升级产生的影响是中断当前写请求，以及主节点重启可能引起的分片分配过程。提升新的主分片一般都比较快，因此对集群的写入可用性影响不大。当索引部分主分片未分配时，使用自动生成ID的情况下，如果持续写入，则客户端对失败重试可能会成功（请求到达已分配成功的主分片），但是会在不同的分片之间产生数据倾斜，倾斜程度视期间数量而定。

写入过程中关闭：线程在写入数据时，会对Engine加写锁。IndicesService的doStop方法对本节点上全部索引并行执行removeIndex，当执行到Engine的flushAndClose（先flush然后关闭Engine），也会对Engine加写锁。由于写入操作已经加了写锁，此时写锁会等待，直到写入执行完毕。因此数据写入过程不会被中断。但是由于网络模块被关闭，客户端的连接会被断开。客户端应当作为失败处理，虽然ES服务端的写流程还在继续。读取过程中关闭：线程在读取数据时，会对Engine加读锁。flushAndClose时的写锁会等待读取过程执行完毕。但是由于连接被关闭，无法发送给客户端，导致客户端读失败。

## 选主流程

Discovery模块负责发现集群中的节点，以及选择主节点。ES支持多种不同Discovery类型选择，内置的实现称为Zen Discovery

但是在相对稳定的对等网络中，主从模式会更好。ES的典型场景中的另一个简化是集群中没有那么多节点。通常，节点的数量远远小于单个节点能够维护的连接数，并且网络环境不必经常处理节点的加入和离开。这就是为什么主从模式更适合ES

Bully算法 它假定所有节点都有一个唯一的ID，使用该ID对节点进行排序。任何时候的当前Leader都是参与集群的最高ID节点

ES 通过推迟选举，直到当前的 Master 失效来解决上述问题，只要当前主节点不挂掉，就不重新选主。但是容易产生脑裂（双主），为此，再通过“法定得票人数过半”解决脑裂问题。

discovery.zen.minimum_master_nodes：最小主节点数，这是防止脑裂、防止数据丢失的极其重要的参数

ZenDiscovery的选主过程如下：· 每个节点计算最小的已知节点ID，该节点为临时Master。向该节点发送领导投票。· 如果一个节点收到足够多的票数，并且该节点也为自己投票，那么它将扮演领导者的角色，开始发布集群状态。所有节点都会参与选举，并参与投票，但是，只有有资格成为Master的节点（node.master为true）的投票才有效．获得多少选票可以赢得选举胜利，就是所谓的法定人数。在 ES 中，法定大小是一个可配置的参数。配置项：discovery.zen.minimum_master_nodes。为了避免脑裂，最小值应该是有Master资格的节点数n/2+1。

在ES中，发送投票就是发送加入集群（JoinRequest）请求。得票就是申请加入该节点的请求的数量。

节点失效检测会监控节点是否离线，然后处理其中的异常。失效检测是选主流程之后不可或缺的步骤，不执行失效检测可能会产生脑裂（双主或多主）。在此我们需要启动两种失效探测器：· 在Master节点，启动NodesFaultDetection，简称NodesFD。定期探测加入集群的节点是否活跃。· 在非Master节点启动MasterFaultDetection，简称MasterFD。定期探测Master节点是否活跃。NodesFaultDetection和MasterFaultDetection都是通过定期（默认为1秒）发送的ping请求探测节点是否正常的，当失败达到一定次数（默认为3次），或者收到来自底层连接模块的节点离线通知时，开始处理节点离开事件。

选主流程在集群中启动，从无主状态到产生新主时执行，同时集群在正常运行过程中， Master探测到节点离开，非Master节点探测到Master离开时都会执行。

## 数据模型

ES的数据副本模型基于主从模式（或称主备模式，HDFS和Cassandra为对等模式），在实现过程中参考了微软的PacificA算法

分片副本使用主从模式。多个副本中存在一个主副本Primary和多个从副本Secondary。所有的数据写入操作都进入主副本，当主副本出现故障无法访问时，系统从其他从副本中选择合适的副本作为新的主副本。

ES 中的每个索引都会被拆分为多个分片，并且每个分片都有多个副本。这些副本称为replication group（副本组，与PacificA中的副本组概念一致），并且在删除或添加文档的时候，各个副本必须同步。否则，从不同副本中读取的数据会不一致。我们把保持分片副本之间的同步，以及从中读取的过程称为数据副本模型

ES的数据副本模型基于主备模式（primary-backup model），主分片是所有索引操作的入口，它负责验证索引操作是否有效。一旦主分片接受一个索引操作，主分片的副分片也会接受该操作。

每个索引操作首先会使用routing参数解析到副本组，通常基于文档ID。一旦确定副本组，就会内部转发该操作到分片组的主分片中。主分片负责验证操作和转发它到其他副分片。

ES维护一个可以接收该操作的分片的副本列表。这个列表叫作同步副本列表（in-sync copies），并由Master节点维护。

写入过程遵循以下基本流程：

（1）请求到达协调节点，协调节点先验证操作，如果有错就拒绝该操作。然后根据当前集群状态，请求被路由到主分片所在节点。

（2）该操作在主分片上本地执行，例如，索引、更新或删除文档。这也会验证字段的内容，如果未通过就拒绝操作（例如，字段串的长度超出Lucene定义的长度）。

（3）操作成功执行后，转发该操作到当前in-sync 副本组的所有副分片。如果有多个副分片，则会并行转发。

（4）一旦所有的副分片成功执行操作并回复主分片，主分片会把请求执行成功的信息返回给协调节点，协调节点返回给客户端。

对于主分片自身错误的情况，它所在的节点会发送一个消息到Master节点。这个索引操作会等待（默认为最多一分钟）Master节点提升一个副分片为主分片。这个操作会被转发给新的主分片。注意，Master同样会监控节点的健康，并且可能会主动降级主分片。这通常发生在主分片所在的节点离线的时候。

在主分片上执行的操作成功后，该主分片必须处理在副分片上潜在发生的错误。错误发生的原因可能是在副分片上执行操作时发生的错误，也可能是因为网络阻塞，导致主分片无法转发操作到副分片，或者副分片无法返回结果给主分片。这些错误都会导致相同的结果：in-sync replica set中的一个分片丢失一个即将要向用户确认的操作。为了避免出现不一致，主分片会发送一条消息到Master节点，要求它把有问题的分片从in-sync replica set中移除。一旦Master确认移除了该分片，主分片就会确认这次操作。注意，Master也会指导另一个节点建立一个新的分片副本，以便把系统恢复成健康状态。

读请求基本流程

（1）把读请求转发到相关分片。注意，因为大多数搜索都会发送到一个或多个索引，通常需要从多个分片中读取，每个分片都保存这些数据的一部分。

（2）从副本组中选择一个相关分片的活跃副本。它可以是主分片或副分片。默认情况下， ES会简单地循环遍历这些分片。

（3）发送分片级的读请求到被选中的副本。

（4）合并结果并给客户端返回响应。注意，针对通过ID查找的get请求，会跳过这个步骤，因为只有一个相关的分片。

单个分片可能降低索引速度因为每次操作时主分片会等待所有在in-sycn列表中的副本，所以单个缓慢的副本可能降低整个副本组的写速度。当然，单个缓慢的分片也会降低读取速度。

脏读从一个被隔离的主分片进行读取，可能读取没有经过确认的写操作。这是因为只有主分片向副分片转发请求，或者向主节点发送请求的时候才会被隔离，此时数据已经在主分片写成功，可以被读取到。ES 通过定期（默认为1秒）“ping”主节点来降低这种风险，如果没有已知的主节点，则拒绝索引操作。

Allocation IDs

ES从5.x版本开始引入Allocation IDs的概念，用于主分片选举策略。每个分片有自己唯一的Allocation ID，同时集群元信息中有一个列表，记录了哪些分片拥有最新数据。

追踪到那个可以安全地被选为主分片的副本，也称之为同步（in-sync）分片副本。

分配决策包含两部分：哪个分片应该分配到哪个节点，以及哪个分片作为主分片，哪些作为副分片。主节点广播集群状态到集群的所有节点。这样每个节点都有了集群状态，它们就可以实现对请求的智能路由。因为每个节点都知道主副分片分配到了哪里。

每个节点都会通过检查集群状态来判断某个分片是否可用。如果一个分片被指定为主分片，则这个节点只需要加载本地分片副本，使之可以用于搜索即可。如果一个分片被分配为副分片，则节点首先需要从主分片所在节点复制差异数据。当集群中可用副分片不足时（在索引设置中指定：index.number_of_replicas），主节点也可以将副分片分配到不含任何此分片副本的节点，从而指示这些节点创建主分片的完整副本。

ES 使用 Allocation IDs 的概念，这是区分不同分片的唯一标识（UUIDS）。

Allocation IDs存储在shard级元信息中，每个shard都有自己唯一的Allocation ID，同时集群级元信息中记录了一个被认为是最新shard的Allocation ID集合，这个集合称为in-sync allocation IDs。

当分配主分片时，主节点检查磁盘中存储的 Allocation ID 是否会在集群状态的 in-sync allocations IDs集合中出现，只有在这个集合中找到了，此分片才有可能被选为主分片。如果活跃副本中的主分片挂了，则in-sync集合中的活跃分片会被提升为主分片，确保集群的写入可用性不变。

处理写请求过程中，当网络产生分区、节点故障，或者部分节点未启动，主分片本地执行完写操作，转发到副分片时，转发操作可能在一个或多个副分片上没能执行成功，这意味着主分片中含有一些没有传播到所有分片的数据，如果这些副分片仍然被认为是同步的，那么即使它们遗漏了一些变化，它们也可能稍后被选为主分片，结果丢失数据。

解决这种问题有两种方法：

（1）让写请求失败，已经写的做回滚处理。

（2）确保差异的（divergent）分片不再被视为同步。

ES 在这种情况下选择了写入可用性：主分片所在节点命令主节点将差异分片的 Allocation IDs从同步集合（in-sync set）中删除。然后，主分片所在节点等待主节点删除成功的确认消息，这个确认消息意味着集群一致层（consensus layer）已成功更新，之后才向客户端确认写请求。这样确保只有包含了所有已确认写入的分片副本才会被主节点选为主分片。

发生严重灾难时，集群中可能会出现只有陈旧副本可用的情况。ES不会把这些分片自动分配为主分片，集群将持续保持Red状态。如果所有in-sync副本都消失了，则集群仍有可能使用陈旧副本进行恢复，但这需要管理员手工干预。

allocate_stale_primary，用于将一个陈旧的分片分配为主分片。使用此命令意味着丢失给定分片副本中缺少的数据。如果同步分片副本只是暂时不可用，则使用此命令意味着会丢失同步分片副本中最近更新的数据。应该把它看作使集群至少运行一些数据的最后一种措施。在所有分片副本都不存在的情况下，还可以强制ES 使用空分片副本分配主分片，这意味着丢失与该分片相关联的所有先前数据。allocate_empty_primary命令只能用于最糟糕的情况

ES从6.0版本开始引入了Sequence IDs概念，使用唯一的ID来标记每个写操作。通过这个ID我们有了索引操作的总排序。

写操作先到达主分片，主分片写完后转发到副分片，在转发到副分片之前，增加一个计数器，为每个操作分配一个序列号是很简单的。但是，由于节点离线随时可能发生，例如，网络分区等，主分片可能被其他副分片取代，仅仅由主分片分配一个序列号无法保证全局唯一性和单调性。因此，我们把当前主分片做一个标记，放到每个操作中，这就是Primary Terms。这样，来自旧的主分片的迟到的操作就可以被检测到然后拒绝（虽然Allocation IDs可以让主分片分配在拥有最新数据的分片上，但仍然可能存在某些情况下主分片上的数据并非最新，例如，手工分配主分片到有旧数据的副本）

Primary Terms和Sequence Numbers

第一步是能够区分新旧两种主分片，我们必须找到一种方法来识别是来自较旧的主分片操作还是来自较新的主分片的操作。最重要的是，整个集群需要达成一致。为此，我们添加了Primary Terms。它由主节点分配，当一个主分片被提升时，Primary Terms递增。然后持久化到集群状态中，从而表示集群主分片所处的一个版本。有了Primary Terms，操作历史中的任何冲突都可以通过查看操作的Primary Terms来解决。新的Terms优先于旧Terms，拒绝过时的操作，避免混乱的情况。

一旦我们有了Primary Terms的保护，就可以添加一个简单的计数器，给每个操作分配一个Sequence Numbers（序列号）。SequenceNumbers使我们能够理解发生在主分片节点上的索引操作的特定顺序

- Primary Terms由主节点分配给每个主分片，每次主分片发生变化时递增。这和Raft中的term，以及Zab中Viewstamped Replication的view-number概念很相似。
- Sequence Numbers标记发生在某个分片上的写操作。由主分片分配，只对写操作分配。假设索引website有2个主分片和1个副分片，当分片website[0]的序列号增加到5时，它的主分片离线，副分片被提升为新的主分片，对于后续写操作，序列号从6开始递增。分片website[1]有自己独立的序列号计数器。主分片每次向副分片转发写请求时，会带上这两个值。为了实现将操作排序，当我们比较两个操作o1和o2时，如果o1 ＜ o2，那么意味着：s1.seq# < s2.seq#或者（s1.seq# == s2.seq# and s1.term < s2.term）

存储成本非常昂贵，直接进行比较的计算工作量太大。为了解决这个问题，ES维护了一个名为“全局检查点”（globalcheckpoint）的安全标记。

全局检查点是所有活跃分片历史都已对齐的序列号，换句话说，所有低于全局检查点的操作都保证已被所有活跃的分片处理完毕。这意味着，当主分片失效时，我们只需要比较新主分片与其他副分片之间的最后一个全局检查点之后的操作即可。当旧主分片恢复时，我们使用它知道的全局检查点，与新主分片进行比较。这样，我们只有小部分操作需要比较，不用比较全部。主分片负责推进全局检查点，它通过跟踪在副分片上完成的操作来实现。一旦它检测到所有副分片已经超出给定序列号，它将相应地更新全局检查点。副分片不会跟踪所有操作，而是维护一个类似全局检查点局部变量，称为本地检查点。

本地检查点也是一个序列号，所有序列号低于它的操作都已在该分片上处理（Lucene 和translog写成功，不一定刷盘）完毕。当副分片确认（ACK）一个写操作到主分片节点时，它们也会更新本地检查点。使用本地检查点，主分片节点能够更新全局检查点，然后在下一次索引操作时将其发送到所有分片副本。全局检测点和本地检查点在内存中维护，但也会保存在每个Lucene提交的元数据中。

当ES恢复一个分片时，需要保证恢复之后与主分片一致。对于冷数据来说，synced flush可以快速验证副分片与主分片是否相同，但对于热数据来说，恢复过程需要从主分片复制整个Lucene分段，如果分段很大，则是非常耗时的操作。现在我们使用副本所知道的最后一个全局检查点，重放来自主分片事务日志（translog）中的相关更改。也就是说，现在可以计算出待恢复分片与主分片数据的差异范围，因此避免复制整个分片。同时，我们多保留一些事务日志（默认为512MB,12小时），直到“太大”或“太老”。如果不能从事务日志恢复，则使用旧的恢复模式。

版本号由主分片生成，在将请求转发给副本片时将携带此版本号。版本号的另一个作用是实现乐观锁，如同其他数据库的乐观锁一样。我们在写请求中指定文档的版本号，如果文档的当前版本与请求中指定的版本号不同，则请求会失败。

## 写流程

## Get流程

## Search流程

## 索引恢复流程

## Snapshot模块

## 问题

如何扩展

数据如何被划分为分片和复制的

索引和搜索时如何进行的

集群中多个节点是如何工作的

主副分片如何同步