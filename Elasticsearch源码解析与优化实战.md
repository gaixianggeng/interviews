# Elasticsearch

## 基本概念原理

Elasticsearch是实时的分布式搜索分析引擎，内部使用Lucene做索引与搜索。

分布式意味着可以动态调整集群规模，弹性扩容，而这一切操作起来都非常简便，用户甚至不必了解集群原理就可以实现。

数据分片以提高水平扩展能力，分布式存储中还会把数据复制成多个副本，放置到不同的机器中，这样一来可以增加系统可用性，同时数据副本还可以使读操作并发执行，分担集群压力。但是多数据副本也带来了一致性的问题：部分副本写成功，部分副本写失败。

ES将数据副本分为主从两部分，即主分片（primaryshard）和副分片（replica shard）。主数据作为权威数据，写过程中先写主分片，成功后再写副分片，恢复阶段以主分片为准。

分片（shard）是底层的基本读写单元，分片的目的是分割巨大索引，让读写可以并行操作，由多台机器共同完成。读写请求最终落到某个分片上，分片可以独立执行读写工作。ES利用分片将数据分发到集群内各处。分片是数据的容器，文档保存在分片内，不会跨分片存储。分片又被分配到集群内的各个节点里。当集群规模扩大或缩小时，ES 会自动在各节点中迁移分片，使数据仍然均匀分布在集群里。

一个ES索引包含很多分片，一个分片是一个Lucene的索引，它本身就是一个完整的搜索引擎，可以独立执行建立索引和搜索任务。Lucene索引又由很多分段组成，每个分段都是一个倒排索引。ES每次“refresh”都会生成一个新的分段，其中包含若干文档的数据。在每个分段内部，文档的不同字段被单独建立索引。

索引建立的时候就需要确定好主分片数 副分片数可以随时修改

搜索1个有着50个分片的索引与搜索50个每个都有1个分片的索引完全等价

以_id 为单位删除文档不会立刻释放空间，删除的 doc 只在 Lucene分段合并时才会真正从磁盘中删除。即使手工触发分段合并，仍然会引起较高的 I/O 压力，并且可能因为分段巨大导致在合并过程中磁盘空间不足（分段大小大于磁盘可用空间的一半）。因此，我们建议周期性地创建新索引。

集群整体分片数量较多，集群管理的总分片数越多压力就越大。可以使用_shrink API来缩减主分片数量，降低集群负载。

倒排索引一旦被写入文件后就具有不变性，不变性具有许多好处：对文件的访问不需要加锁，读取索引时可以被文件系统缓存等。

新增内容并写到一个新的倒排索引中，查询时，每个倒排索引都被轮流查询，查询完再对结果进行合并。每次内存缓冲的数据被写入文件时，会产生一个新的Lucene段，每个段都是一个倒排索引。在一个记录元信息的文件中描述了当前Lucene索引都含有哪些分段。由于分段的不变性，更新、删除等操作实际上是将数据标记为删除，记录到单独的位置，这种方式称为标记删除。因此删除部分数据不会释放磁盘空间。

近实时搜索

一般情况下（direct方式除外），通过操作系统write接口写到磁盘的数据先到达系统缓存（内存）,write函数返回成功时，数据未必被刷到磁盘。通过手工调用flush，或者操作系统通过一定策略将系统缓存刷到磁盘。这种策略大幅提升了写入效率。从write函数返回成功开始，无论数据有没有被刷到磁盘，该数据已经对读取可见。

每秒产生一个新分段，新段先写入文件系统缓存，但稍后再执行flush刷盘操作，写操作很快会执行完，一旦写成功，就可以像其他文件一样被打开和读取了。

通用的做法是记录事务日志，每次对ES进行操作时均记录事务日志，当ES启动的时候，重放translog中所有在最后一次提交后发生的变更操作。

在ES中，每秒清空一次写缓冲，将这些数据写入文件，这个过程称为refresh，每次refresh会创建一个新的Lucene 段。但是分段数量太多会带来较大的麻烦，每个段都会消耗文件句柄、内存。

在ES中，每秒清空一次写缓冲，将这些数据写入文件，这个过程称为refresh，每次refresh会创建一个新的Lucene 段。但是分段数量太多会带来较大的麻烦，每个段都会消耗文件句柄、内存。

段越多，搜索也就越慢 常用的方案是选择大小相似的分段进行合并。在合并过程中，标记为删除的数据不会写入新分段，当合并过程结束，旧的分段数据被删除，标记删除的数据才从磁盘删除。

如果段文件设置一定上限不再合并，则对表中部分数据无法实现真正的物理删除。ES存在同样的问题。

es的refresh是lucene的flush，es的flush是lucene的commit

主从模式可以简化系统设计，Master作为权威节点，部分操作仅由Master执行，并负责维护集群元信息。缺点是Master节点存在单点故障，需要解决灾备问题，并且集群规模会受限于Master节点的管理能力。

主节点 数据节点 预处理节点 协调节点

集群健康状态分为三种：· Green，所有的主分片和副分片都正常运行。· Yellow，所有的主分片都正常运行，但不是所有的副分片都正常运行。这意味着存在单点故障风险。· Red，有主分片没能正常运行。每个索引也有上述三种状态

集群状态元数据是全局信息，元数据包括内容路由信息、配置信息等，其中最重要的是内容路由信息，它描述了“哪个分片位于哪个节点”这种信息。

集群状态由主节点负责维护，如果主节点从数据节点接收更新，则将这些更新广播到集群的其他节点，让每个节点上的集群状态保持最新。

分片分配过程中除了让节点间均匀存储，还要保证不把主分片和副分片分配到同一节点，避免单个节点故障引起数据丢失。

ES中几个基础模块的功能

Cluster模块是主节点执行集群管理的封装实现，管理集群状态，维护集群层面的配置信息。

allocation封装了分片分配相关的功能和策略，包括主分片的分配和副分片的分配，本模块由主节点调用。创建新索引、集群完全重启都需要分片分配的过程。

Discovery发现模块负责发现集群中的节点，以及选举主节点。当节点加入或退出集群时，主节点会采取相应的行动。zookeeper

gateway负责对收到Master广播下来的集群状态（cluster state）数据的持久化存储，并在集群完全重启时恢复它们。

Transport传输模块用于集群内节点之间的内部通信。从一个节点到另一个节点的每个请求都使用传输模块。异步

Engine模块封装了对Lucene的操作及translog的调用，它是对一个分片读写操作的最终提供者

## 集群启动流程

经历选举主节点、主分片、数据恢复等重要阶段

ES的选主算法是基于Bully算法的改进，主要思路是对节点ID排序，取ID值最大的节点作为Master

三个附加约定条件

- 参选人数需要过半，达到 quorum（多数）后就选出了临时的主
- 得票数需过半。某节点被选为主节点，必须判断加入它的节点数过半，才确认Master身份。
- 当探测到节点离开事件时，必须判断当前节点数是否过半。如果达不到quorum，则放弃Master身份，重新加入集群。 防止脑裂

discovery.zen.minimum_master_nodes

被选出的 Master 和集群元信息的新旧程度没有关系。因此它的第一个任务是选举元信息，让各节点把各自存储的元信息发过来，根据版本号确定最新的元信息，然后把这个信息广播下去，这样集群的所有节点都有了最新的元信息

集群元信息的选举包括两个级别：集群级和索引级。不包含哪个shard存于哪个节点这种信息。这种信息以节点磁盘存储的为准，需要上报。因为读写流程是不经过Master的，Master 不知道各 shard 副本直接的数据差异。

allocation过程

选举shard级元信息，构建内容路由表，是在allocation模块完成的。在初始阶段，所有的shard都处于UNASSIGNED（未分配）状态。ES中通过分配过程决定哪个分片位于哪个节点，重构内容路由表。此时，首先要做的是分配主分片。

所有的分配工作都是 Master 来做的，此时， Master不知道主分片在哪，它向集群的所有节点询问：大家把[website][0]分片的元信息发给我。然后，Master 等待所有的请求返回，正常情况下它就有了这个 shard 的信息，然后根据某种策略选一个分片作为主分片。是不是效率有些低？这种询问量=shard 数×节点数。所以说我们最好控制shard的总规模别太大。

从ES 5.x开始，主分片选举过程是通过集群级元信息中记录的“最新主分片的列表”来确定主分片的：汇报信息中存在，并且这个列表中也存在。

如果集群设置了：＂cluster.routing.allocation.enable＂: ＂none＂禁止分配分片，集群仍会强制分配主分片。因此，在设置了上述选项的情况下，集群重启后的状态为Yellow，而非Red。

allocation过程中允许新启动的节点加入集群。

index recovery

分片分配成功后进入recovery流程。主分片的recovery不会等待其副分片分配成功才开始recovery。它们是独立的流程，只是副分片的recovery需要主分片恢复完毕才开始。

为什么需要recovery？对于主分片来说，可能有一些数据没来得及刷盘；对于副分片来说，一是没刷盘，二是主分片写完了，副分片还没来得及写，主副分片数据不一致。

1． 主分片recovery由于每次写操作都会记录事务日志（translog），事务日志中记录了哪种操作，以及相关的数据。因此将最后一次提交（Lucene 的一次提交就是一次 fsync 刷盘的过程）之后的 translog中进行重放，建立Lucene索引，如此完成主分片的recovery。

2． 副分片recovery副分片的恢复是比较复杂的，在ES的版本迭代中，副分片恢复策略有过不少调整。副分片需要恢复成与主分片一致，同时，恢复期间允许新的索引操作。在目前的6.0版本中，恢复分成两阶段执行。· phase1：在主分片所在节点，获取translog保留锁，从获取保留锁开始，会保留translog不受其刷盘清空的影响。然后调用Lucene接口把shard做快照，这是已经刷磁盘中的分片数据。把这些shard数据复制到副本节点。在phase1完毕前，会向副分片节点发送告知对方启动engine，在phase2开始之前，副分片就可以正常处理写请求了。· phase2：对translog做快照，这个快照里包含从phase1开始，到执行translog快照期间的新增索引。将这些translog发送到副分片所在节点进行重放。

由于需要支持恢复期间的新增写操作（让ES的可用性更强），这两个阶段中需要重点关注以下几个问题。

- 分片数据完整性：如何做到副分片不丢数据？第二阶段的 translog 快照包括第一阶段所有的新增操作。那么第一阶段执行期间如果发生“Lucene commit”（将文件系统写缓冲中的数据刷盘，并清空translog），清除translog怎么办？在ES2.0之前，是阻止了刷新操作，以此让translog都保留下来。从2.0版本开始，为了避免这种做法产生过大的translog，引入了translog.view的概念，创建 view 可以获取后续的所有操作。从6.0版本开始，translog.view 被移除。引入TranslogDeletionPolicy的概念，它将translog做一个快照来保持translog不被清理。这样实现了在第一阶段允许Lucene commit。
- 数据一致性：在ES 2.0之前，副分片恢复过程有三个阶段，第三阶段会阻塞新的索引操作，传输第二阶段执行期间新增的translog，这个时间很短。自2.0版本之后，第三阶段被删除，恢复期间没有任何写阻塞过程。在副分片节点，重放translog时，phase1和phase2之间的写操作与phase2重放操作之间的时序错误和冲突，通过写流程中进行异常处理，对比版本号来过滤掉过期操作。这样，时序上存在错误的操作被忽略，对于特定的 doc，只有最新一次操作生效，保证了主副分片一致。

第一阶段尤其漫长，因为它需要从主分片拉取全量的数据。在ES 6.x中，对第一阶段再次优化：标记每个操作。在正常的写操作中，每次写入成功的操作都分配一个序号，通过对比序号就可以计算出差异范围，在实现方式上，添加了globalcheckpoint和local checkpoint，主分片负责维护global checkpoint，代表所有分片都已写入这个序号的位置，local checkpoint代表当前分片已写入成功的最新位置，恢复时通过对比两个序列号，计算出缺失的数据范围，然后通过translog重放这部分数据，同时translog会为此保留更长的时间。因此，有两个机会可以跳过副分片恢复的phase1：基于SequenceNumber，从主分片节点的translog恢复数据；主副两分片有相同的syncid且doc数相同，可以跳过phase1。

当一个索引的主分片分配成功后，到此分片的写操作就是允许的。当一个索引所有的主分片都分配成功后，该索引变为Yellow。当全部索引的主分片都分配成功后，整个集群变为Yellow。当一个索引全部分片分配成功后，该索引变为 Green。当全部索引的索引分片分配成功后，整个集群变为Green。

索引数据恢复是最漫长的过程。当shard总量达到十万级的时候，6.x之前的版本集群从Red变为Green的时间可能需要小时级。ES 6.x中的副本允许从本地translog恢复是一次重大的改进，避免了从主分片所在节点拉取全量数据，为恢复过程节约了大量时间。

## 节点启动和关闭

检测外部环境 堆大小、文件描述符、内存锁定、最大线程数等

内部模块启动，启动子模块，初始化内部数据、创建线程池、启动线程池

启动keepalive线程 调用keepAliveThread.start（）方法启动keepalive线程，线程本身不做具体的工作。主线程执行完启动流程后会退出，keepalive线程是唯一的用户线程，作用是保持进程运行。在Java程序中，至少要有一个用户线程。当用户线程数为零时退出进程。

ES进程会捕获SIGTERM信号（kill命令默认信号）进行处理，调用各模块的stop方法，让它们有机会停止服务，安全退出。进程重启期间，如果主节点被关闭，则集群会重新选主，在这期间，集群有一个短暂的无主状态。如果集群中的主节点是单独部署的，则新主当选后，可以跳过gateway和recovery流程，否则新主需要重新分配旧主所持有的分片：提升其他副本为主分片，以及分配新的副分片。如果数据节点被关闭，则读写请求的TCP连接也会因此关闭，对客户端来说写操作执行失败。但写流程已经到达Engine环节的会正常写完，只是客户端无法感知结果。此时客户端重试，如果使用自动生成ID，则数据内容会重复。

滚动升级产生的影响是中断当前写请求，以及主节点重启可能引起的分片分配过程。提升新的主分片一般都比较快，因此对集群的写入可用性影响不大。当索引部分主分片未分配时，使用自动生成ID的情况下，如果持续写入，则客户端对失败重试可能会成功（请求到达已分配成功的主分片），但是会在不同的分片之间产生数据倾斜，倾斜程度视期间数量而定。

写入过程中关闭：线程在写入数据时，会对Engine加写锁。IndicesService的doStop方法对本节点上全部索引并行执行removeIndex，当执行到Engine的flushAndClose（先flush然后关闭Engine），也会对Engine加写锁。由于写入操作已经加了写锁，此时写锁会等待，直到写入执行完毕。因此数据写入过程不会被中断。但是由于网络模块被关闭，客户端的连接会被断开。客户端应当作为失败处理，虽然ES服务端的写流程还在继续。读取过程中关闭：线程在读取数据时，会对Engine加读锁。flushAndClose时的写锁会等待读取过程执行完毕。但是由于连接被关闭，无法发送给客户端，导致客户端读失败。

## 选主流程

Discovery模块负责发现集群中的节点，以及选择主节点。ES支持多种不同Discovery类型选择，内置的实现称为Zen Discovery

但是在相对稳定的对等网络中，主从模式会更好。ES的典型场景中的另一个简化是集群中没有那么多节点。通常，节点的数量远远小于单个节点能够维护的连接数，并且网络环境不必经常处理节点的加入和离开。这就是为什么主从模式更适合ES

Bully算法 它假定所有节点都有一个唯一的ID，使用该ID对节点进行排序。任何时候的当前Leader都是参与集群的最高ID节点

ES 通过推迟选举，直到当前的 Master 失效来解决上述问题，只要当前主节点不挂掉，就不重新选主。但是容易产生脑裂（双主），为此，再通过“法定得票人数过半”解决脑裂问题。

discovery.zen.minimum_master_nodes：最小主节点数，这是防止脑裂、防止数据丢失的极其重要的参数

ZenDiscovery的选主过程如下：· 每个节点计算最小的已知节点ID，该节点为临时Master。向该节点发送领导投票。· 如果一个节点收到足够多的票数，并且该节点也为自己投票，那么它将扮演领导者的角色，开始发布集群状态。所有节点都会参与选举，并参与投票，但是，只有有资格成为Master的节点（node.master为true）的投票才有效．获得多少选票可以赢得选举胜利，就是所谓的法定人数。在 ES 中，法定大小是一个可配置的参数。配置项：discovery.zen.minimum_master_nodes。为了避免脑裂，最小值应该是有Master资格的节点数n/2+1。

在ES中，发送投票就是发送加入集群（JoinRequest）请求。得票就是申请加入该节点的请求的数量。

节点失效检测会监控节点是否离线，然后处理其中的异常。失效检测是选主流程之后不可或缺的步骤，不执行失效检测可能会产生脑裂（双主或多主）。在此我们需要启动两种失效探测器：· 在Master节点，启动NodesFaultDetection，简称NodesFD。定期探测加入集群的节点是否活跃。· 在非Master节点启动MasterFaultDetection，简称MasterFD。定期探测Master节点是否活跃。NodesFaultDetection和MasterFaultDetection都是通过定期（默认为1秒）发送的ping请求探测节点是否正常的，当失败达到一定次数（默认为3次），或者收到来自底层连接模块的节点离线通知时，开始处理节点离开事件。

选主流程在集群中启动，从无主状态到产生新主时执行，同时集群在正常运行过程中， Master探测到节点离开，非Master节点探测到Master离开时都会执行。

## 数据模型

ES的数据副本模型基于主从模式（或称主备模式，HDFS和Cassandra为对等模式），在实现过程中参考了微软的PacificA算法

分片副本使用主从模式。多个副本中存在一个主副本Primary和多个从副本Secondary。所有的数据写入操作都进入主副本，当主副本出现故障无法访问时，系统从其他从副本中选择合适的副本作为新的主副本。

ES 中的每个索引都会被拆分为多个分片，并且每个分片都有多个副本。这些副本称为replication group（副本组，与PacificA中的副本组概念一致），并且在删除或添加文档的时候，各个副本必须同步。否则，从不同副本中读取的数据会不一致。我们把保持分片副本之间的同步，以及从中读取的过程称为数据副本模型

ES的数据副本模型基于主备模式（primary-backup model），主分片是所有索引操作的入口，它负责验证索引操作是否有效。一旦主分片接受一个索引操作，主分片的副分片也会接受该操作。

每个索引操作首先会使用routing参数解析到副本组，通常基于文档ID。一旦确定副本组，就会内部转发该操作到分片组的主分片中。主分片负责验证操作和转发它到其他副分片。

ES维护一个可以接收该操作的分片的副本列表。这个列表叫作同步副本列表（in-sync copies），并由Master节点维护。

写入过程遵循以下基本流程：

（1）请求到达协调节点，协调节点先验证操作，如果有错就拒绝该操作。然后根据当前集群状态，请求被路由到主分片所在节点。

（2）该操作在主分片上本地执行，例如，索引、更新或删除文档。这也会验证字段的内容，如果未通过就拒绝操作（例如，字段串的长度超出Lucene定义的长度）。

（3）操作成功执行后，转发该操作到当前in-sync 副本组的所有副分片。如果有多个副分片，则会并行转发。

（4）一旦所有的副分片成功执行操作并回复主分片，主分片会把请求执行成功的信息返回给协调节点，协调节点返回给客户端。

对于主分片自身错误的情况，它所在的节点会发送一个消息到Master节点。这个索引操作会等待（默认为最多一分钟）Master节点提升一个副分片为主分片。这个操作会被转发给新的主分片。注意，Master同样会监控节点的健康，并且可能会主动降级主分片。这通常发生在主分片所在的节点离线的时候。

在主分片上执行的操作成功后，该主分片必须处理在副分片上潜在发生的错误。错误发生的原因可能是在副分片上执行操作时发生的错误，也可能是因为网络阻塞，导致主分片无法转发操作到副分片，或者副分片无法返回结果给主分片。这些错误都会导致相同的结果：in-sync replica set中的一个分片丢失一个即将要向用户确认的操作。为了避免出现不一致，主分片会发送一条消息到Master节点，要求它把有问题的分片从in-sync replica set中移除。一旦Master确认移除了该分片，主分片就会确认这次操作。注意，Master也会指导另一个节点建立一个新的分片副本，以便把系统恢复成健康状态。

读请求基本流程

（1）把读请求转发到相关分片。注意，因为大多数搜索都会发送到一个或多个索引，通常需要从多个分片中读取，每个分片都保存这些数据的一部分。

（2）从副本组中选择一个相关分片的活跃副本。它可以是主分片或副分片。默认情况下， ES会简单地循环遍历这些分片。

（3）发送分片级的读请求到被选中的副本。

（4）合并结果并给客户端返回响应。注意，针对通过ID查找的get请求，会跳过这个步骤，因为只有一个相关的分片。

单个分片可能降低索引速度因为每次操作时主分片会等待所有在in-sycn列表中的副本，所以单个缓慢的副本可能降低整个副本组的写速度。当然，单个缓慢的分片也会降低读取速度。

脏读从一个被隔离的主分片进行读取，可能读取没有经过确认的写操作。这是因为只有主分片向副分片转发请求，或者向主节点发送请求的时候才会被隔离，此时数据已经在主分片写成功，可以被读取到。ES 通过定期（默认为1秒）“ping”主节点来降低这种风险，如果没有已知的主节点，则拒绝索引操作。

Allocation IDs

ES从5.x版本开始引入Allocation IDs的概念，用于主分片选举策略。每个分片有自己唯一的Allocation ID，同时集群元信息中有一个列表，记录了哪些分片拥有最新数据。

追踪到那个可以安全地被选为主分片的副本，也称之为同步（in-sync）分片副本。

分配决策包含两部分：哪个分片应该分配到哪个节点，以及哪个分片作为主分片，哪些作为副分片。主节点广播集群状态到集群的所有节点。这样每个节点都有了集群状态，它们就可以实现对请求的智能路由。因为每个节点都知道主副分片分配到了哪里。

每个节点都会通过检查集群状态来判断某个分片是否可用。如果一个分片被指定为主分片，则这个节点只需要加载本地分片副本，使之可以用于搜索即可。如果一个分片被分配为副分片，则节点首先需要从主分片所在节点复制差异数据。当集群中可用副分片不足时（在索引设置中指定：index.number_of_replicas），主节点也可以将副分片分配到不含任何此分片副本的节点，从而指示这些节点创建主分片的完整副本。

ES 使用 Allocation IDs 的概念，这是区分不同分片的唯一标识（UUIDS）。

Allocation IDs存储在shard级元信息中，每个shard都有自己唯一的Allocation ID，同时集群级元信息中记录了一个被认为是最新shard的Allocation ID集合，这个集合称为in-sync allocation IDs。

当分配主分片时，主节点检查磁盘中存储的 Allocation ID 是否会在集群状态的 in-sync allocations IDs集合中出现，只有在这个集合中找到了，此分片才有可能被选为主分片。如果活跃副本中的主分片挂了，则in-sync集合中的活跃分片会被提升为主分片，确保集群的写入可用性不变。

处理写请求过程中，当网络产生分区、节点故障，或者部分节点未启动，主分片本地执行完写操作，转发到副分片时，转发操作可能在一个或多个副分片上没能执行成功，这意味着主分片中含有一些没有传播到所有分片的数据，如果这些副分片仍然被认为是同步的，那么即使它们遗漏了一些变化，它们也可能稍后被选为主分片，结果丢失数据。

解决这种问题有两种方法：

（1）让写请求失败，已经写的做回滚处理。

（2）确保差异的（divergent）分片不再被视为同步。

ES 在这种情况下选择了写入可用性：主分片所在节点命令主节点将差异分片的 Allocation IDs从同步集合（in-sync set）中删除。然后，主分片所在节点等待主节点删除成功的确认消息，这个确认消息意味着集群一致层（consensus layer）已成功更新，之后才向客户端确认写请求。这样确保只有包含了所有已确认写入的分片副本才会被主节点选为主分片。

发生严重灾难时，集群中可能会出现只有陈旧副本可用的情况。ES不会把这些分片自动分配为主分片，集群将持续保持Red状态。如果所有in-sync副本都消失了，则集群仍有可能使用陈旧副本进行恢复，但这需要管理员手工干预。

allocate_stale_primary，用于将一个陈旧的分片分配为主分片。使用此命令意味着丢失给定分片副本中缺少的数据。如果同步分片副本只是暂时不可用，则使用此命令意味着会丢失同步分片副本中最近更新的数据。应该把它看作使集群至少运行一些数据的最后一种措施。在所有分片副本都不存在的情况下，还可以强制ES 使用空分片副本分配主分片，这意味着丢失与该分片相关联的所有先前数据。allocate_empty_primary命令只能用于最糟糕的情况

ES从6.0版本开始引入了Sequence IDs概念，使用唯一的ID来标记每个写操作。通过这个ID我们有了索引操作的总排序。

写操作先到达主分片，主分片写完后转发到副分片，在转发到副分片之前，增加一个计数器，为每个操作分配一个序列号是很简单的。但是，由于节点离线随时可能发生，例如，网络分区等，主分片可能被其他副分片取代，仅仅由主分片分配一个序列号无法保证全局唯一性和单调性。因此，我们把当前主分片做一个标记，放到每个操作中，这就是Primary Terms。这样，来自旧的主分片的迟到的操作就可以被检测到然后拒绝（虽然Allocation IDs可以让主分片分配在拥有最新数据的分片上，但仍然可能存在某些情况下主分片上的数据并非最新，例如，手工分配主分片到有旧数据的副本）

Primary Terms和Sequence Numbers

第一步是能够区分新旧两种主分片，我们必须找到一种方法来识别是来自较旧的主分片操作还是来自较新的主分片的操作。最重要的是，整个集群需要达成一致。为此，我们添加了Primary Terms。它由主节点分配，当一个主分片被提升时，Primary Terms递增。然后持久化到集群状态中，从而表示集群主分片所处的一个版本。有了Primary Terms，操作历史中的任何冲突都可以通过查看操作的Primary Terms来解决。新的Terms优先于旧Terms，拒绝过时的操作，避免混乱的情况。

一旦我们有了Primary Terms的保护，就可以添加一个简单的计数器，给每个操作分配一个Sequence Numbers（序列号）。SequenceNumbers使我们能够理解发生在主分片节点上的索引操作的特定顺序

- Primary Terms由主节点分配给每个主分片，每次主分片发生变化时递增。这和Raft中的term，以及Zab中Viewstamped Replication的view-number概念很相似。
- Sequence Numbers标记发生在某个分片上的写操作。由主分片分配，只对写操作分配。假设索引website有2个主分片和1个副分片，当分片website[0]的序列号增加到5时，它的主分片离线，副分片被提升为新的主分片，对于后续写操作，序列号从6开始递增。分片website[1]有自己独立的序列号计数器。主分片每次向副分片转发写请求时，会带上这两个值。为了实现将操作排序，当我们比较两个操作o1和o2时，如果o1 ＜ o2，那么意味着：s1.seq# < s2.seq#或者（s1.seq# == s2.seq# and s1.term < s2.term）

存储成本非常昂贵，直接进行比较的计算工作量太大。为了解决这个问题，ES维护了一个名为“全局检查点”（globalcheckpoint）的安全标记。

全局检查点是所有活跃分片历史都已对齐的序列号，换句话说，所有低于全局检查点的操作都保证已被所有活跃的分片处理完毕。这意味着，当主分片失效时，我们只需要比较新主分片与其他副分片之间的最后一个全局检查点之后的操作即可。当旧主分片恢复时，我们使用它知道的全局检查点，与新主分片进行比较。这样，我们只有小部分操作需要比较，不用比较全部。主分片负责推进全局检查点，它通过跟踪在副分片上完成的操作来实现。一旦它检测到所有副分片已经超出给定序列号，它将相应地更新全局检查点。副分片不会跟踪所有操作，而是维护一个类似全局检查点局部变量，称为本地检查点。

本地检查点也是一个序列号，所有序列号低于它的操作都已在该分片上处理（Lucene 和translog写成功，不一定刷盘）完毕。当副分片确认（ACK）一个写操作到主分片节点时，它们也会更新本地检查点。使用本地检查点，主分片节点能够更新全局检查点，然后在下一次索引操作时将其发送到所有分片副本。全局检测点和本地检查点在内存中维护，但也会保存在每个Lucene提交的元数据中。

当ES恢复一个分片时，需要保证恢复之后与主分片一致。对于冷数据来说，synced flush可以快速验证副分片与主分片是否相同，但对于热数据来说，恢复过程需要从主分片复制整个Lucene分段，如果分段很大，则是非常耗时的操作。现在我们使用副本所知道的最后一个全局检查点，重放来自主分片事务日志（translog）中的相关更改。也就是说，现在可以计算出待恢复分片与主分片数据的差异范围，因此避免复制整个分片。同时，我们多保留一些事务日志（默认为512MB,12小时），直到“太大”或“太老”。如果不能从事务日志恢复，则使用旧的恢复模式。

版本号由主分片生成，在将请求转发给副本片时将携带此版本号。版本号的另一个作用是实现乐观锁，如同其他数据库的乐观锁一样。我们在写请求中指定文档的版本号，如果文档的当前版本与请求中指定的版本号不同，则请求会失败。

## 写流程

单个文档的请求称为Index请求，批量写入的请求称为Bulk请求。写单个和多个文档使用相同的处理逻辑

CREATE:put 请求可以通过 op_type 参数设置操作类型为 create，在这种操作下，如果文档已存在，则请求将失败

UPDATE：默认情况下，“put”一个文档时，如果文档已存在，则更新它。

es默认使用文档id进行路由，也可指定routing

写操作必须先在主分片执行成功后才能复制到相关的副分片。

写单个文档所需的步骤：

（1）客户端向NODE1发送写请求。

（2）NODE1使用文档ID来确定文档属于分片0，通过集群状态中的内容路由表信息获知分片0的主分片位于NODE3，因此请求被转发到NODE3上。

（3）NODE3上的主分片执行写操作。如果写入成功，则它将请求并行转发到NODE1和NODE2的副分片上，等待返回结果。当所有的副分片都报告成功，NODE3将向协调节点报告成功，协调节点再向客户端报告成功。

默认执行写入前，只要主分片处于活跃状态就可以写入，wait_for_active_shards用于指定开始执行写入操作前需要等待的活跃分片数量

每个响应也是以shard为单位的。如果某个shard的响应中部分doc写失败了，则将异常信息填充到Response中，整体请求做成功处理。

主分片节点流程

1. 检查请求
2. 是否延迟执行，是的话放入队列
3. 是否主分片已发生转移
4. 检测写一致性，默认主分片可用就执行写入
5. 写lucene后写translog
6. flush translog，根据策略刷盘
7. 写副分片

副分片节点流程

1. 执行与主分片基本相同的写doc过程，写完毕后回复主分片节点

异常流程总结

1. 如果请求在协调节点的路由阶段失败，则会等待集群状态更新，拿到更新后，进行重试，如果再次失败，则仍旧等集群状态更新，直到超时1分钟为止。超时后仍失败则进行整体请求失败处理。
2. 在主分片写入过程中，写入是阻塞的。只有写入成功，才会发起写副本请求。如果主shard写失败，则整个请求被认为处理失败。如果有部分副本写失败，则整个请求被认为处理成功。
3. 无论主分片还是副分片，当写一个doc失败时，集群不会重试，而是关闭本地shard，然后向Master汇报，删除是以shard为单位的。
- 数据可靠性：通过分片副本和事务日志机制保障数据安全。
- 服务可用性：在可用性和一致性的取舍方面，默认情况下 ES 更倾向于可用性，只要主分片可用即可执行写入操作。
- 一致性：笔者认为是弱一致性。只要主分片写成功，数据就可能被读取。因此读取操作在主分片和副分片上可能会得到不同结果
- 原子性：索引的读写、别名更新是原子操作，不会出现中间状态。但bulk不是原子操作，不能用来实现事务
- 扩展性：主副分片都可以承担读请求，分担系统负载。

## Get流程

get默认是实时的

根据内容路由算法计算目标shardid，也就是文档应该落在哪个分片上

在5.x版本之前，GET/MGET的实时读取依赖于从translog中读取实现，5.x版本之后的版本改为refresh，因此系统对实时读取的支持会对写入速度有负面影响。

update操作需要先GET再写，为了保证一致性，update调用GET时将realtime选项设置为true，并且不可配置。因此update操作可能会导致refresh生成新的Lucene分段。

## Search流程

找到匹配文档仅仅完成了搜索流程的一半，因为多分片中的结果必须组合成单个排序列表。集群的任意节点都可以接收搜索请求，接收客户端请求的节点称为协调节点。在协调节点，搜索任务被执行成一个两阶段过程，即query then fetch。真正执行搜索任务的节点称为数据节点。

需要两个阶段才能完成搜索的原因是，在查询的时候不知道文档位于哪个分片，因此索引的所有分片（某个副本）都要参与搜索，然后协调节点将结果合并，再根据文档ID获取文档内容。例如，有5个分片，查询返回前10个匹配度最高的文档，那么每个分片都查询出当前分片的TOP 10，协调节点将5×10 = 50的结果再次排序，返回最终TOP 10的结果给客户端。

搜索调用Lucene完成，如果是全文检索

1.  对检索字段使用建立索引时相同的分析器进行分析，产生Token列表；
2. 根据查询语句的语法规则转换成一棵语法树；
3. 查找符合语法树的文档；
4. 对匹配到的文档列表进行相关性评分，评分策略一般使用TF/IDF；
5. 根据评分结果进行排序。

两种不同的搜索类型的区别在于查询阶段，DFS查询阶段的流程要多一些，它使用全局信息来获取更准确的评分。

Query阶段

在初始查询阶段，查询会广播到索引中每一个分片副本（主分片或副分片）。

每个分片在本地执行搜索并构建一个匹配文档的优先队列。

优先队列是一个存有topN匹配文档的有序列表。优先队列大小为分页参数from +size。

Fetch阶段由以下步骤构成：

1. 协调节点向相关NODE发送GET请求。
2. 分片所在节点向协调节点返回数据。
3. 协调节点等待所有文档被取得，然后返回给客户端。分片所在节点在返回文档数据时，处理有可能出现的_source字段和高亮参数

Fetch阶段的目的是通过文档ID获取完整的文档内容

- 聚合是在ES中实现的，而非Lucene。
- Query和Fetch请求之间是无状态的，除非是scroll方式。
- 分页搜索不会单独“cache”,cache和分页没有关系。
- 搜索需要遍历分片所有的Lucene分段，因此合并Lucene分段对搜索性能有好处。

## 索引恢复流程

索引恢复（indices.recovery）是ES数据恢复过程。待恢复的数据是客户端写入成功，但未执行刷盘（flush）的Lucene分段。例如，当节点异常重启时，写入磁盘的数据先到文件系统的缓冲，未必来得及刷盘，如果不通过某种方式将未刷盘的数据找回来，则会丢失一些数据，这是保持数据完整性的体现；另一方面，由于写入操作在多个分片副本上没有来得及全部执行，副分片需要同步成和主分片完全一致，这是数据副本一致性的体现。

根据数据分片性质，索引恢复过程可分为主分片恢复流程和副分片恢复流程。

- 主分片从translog中自我恢复，尚未执行flush到磁盘的Lucene分段可以从translog中重建；
- 副分片需要从主分片中拉取Lucene分段和translog进行恢复。但是有机会跳过拉取Lucene分段的过程。索引恢复的触发条件包括从快照备份恢复、节点加入和离开、索引的_open操作等。

主分片恢复关键是translog阶段

一个Lucene索引由许多分段组成，每次搜索时遍历所有分段。内部维护了一个称为“提交点”的信息，其描述了当前Lucene索引都包括哪些分段，这些分段已经被fsync系统调用，从操作系统的cache刷入磁盘。每次提交操作都会将分段刷入磁盘实现持久化。

副分片恢复流程

副分片恢复的核心思想是从主分片拉取Lucene分段和translog进行恢复。按数据传递的方向，主分片节点称为Source，副分片节点称为Target。为什么需要拉取主分片的translog？因为在副分片恢复期间允许新的写操作，从复制Lucene分段的那一刻开始，所恢复的副分片数据不包括新增的内容，而这些内容存在于主分片的translog中，因此副分片需要从主分片节点拉取translog进行重放，以获取新增内容。这就需要主分片节点的translog不被清理。

因此从6.0版本开始，translog.view被移除。引入TranslogDeletionPolicy的概念，负责维护活跃（liveness）的translog文件。这个类的实现非常简单，它将translog做一个快照来保持translog不被清理。这样使用者只需创建一个快照，无须担心视图之类。恢复流程实际上确实需要一个视图，现在可以通过获取一个简单的保留锁来防止清理translog。这消除了视图概念的需求。

在保证translog不被清理后，恢复核心处理过程由两个内部阶段（phase）组成。

- phase1：在主分片所在节点，获取translog保留锁，从获取保留锁开始，会保留translog不受其刷盘清空的影响。然后调用Lucene接口把shard做快照，快照含有shard中已经刷到磁盘的文件引用，把这些shard数据复制到副本节点。在phase1结束前，会向副分片节点发送告知对方启动Engine，在phase2开始之前，副分片就可以正常处理写请求了。
- phase2：对translog做快照，这个快照里包含从phase1开始，到执行translog快照期间的新增索引。将这些translog发送到副分片所在节点进行重放

由于phase1需要通过网络复制大量数据，过程非常漫长，在ES 6.x中，有两个机会可以跳过phase1：（1）如果可以基于恢复请求中的SequenceNumber进行恢复，则跳过phase1。（2）如果主副两分片有相同的syncid且doc数相同，则跳过phase1。

synced flush，默认情况下5分钟没有写入操作的索引被标记为inactive，执行synced flush，生成一个唯一的syncid，写入分片的所有副本中。这个syncid是分片级，意味着拥有相同syncid的分片具有相同的Lucene索引。

synced flush本质上是一次普通的flush操作，只是在Lucene的commit过程中多写了一个syncid。原则上，在没有数据写入的情况下，各分片在同一时间“flush”成功后，它们理应有相同的Lucene索引内容，无论Lucene分段是否一致。于是给分片分配一个id，表示数据一致。

phase1检查目标节点上的段文件，并对缺失的部分进行复制。只有具有相同大小和校验和的段才能被重用。但是由于分片副本执行各自的合并策略，所以合并出来的段文件相同的概率很低。

phase2将translog批量发送到副分片节点，发送时将待发送的translog组合成一批来提高发送效率，默认的批量大小为512KB，不支持配置。

recovery速度优化

众所周知，索引恢复是集群启动过程中最缓慢的过程，集群完全重启，或者Master节点挂掉后，新选出的Master也有可能执行这个过程。

官方也一直在优化索引恢复速度，陆续添加了syncid和SequenceNumber。下面归纳一下有哪些方法可以提升索引恢复速度：

- 配置项 cluster.routing.allocation.node_concurrent_recoveries 决定了单个节点执行副分片recovery时的最大并发数（进/出），默认为2，适当提高此值可以增加recovery并发数。
- 配置项indices.recovery.max_bytes_per_sec决定节点间复制数据时的限速，可以适当提高此值或取消限速。
- 配置项cluster.routing.allocation.node_initial_primaries_recoveries决定了单个节点执行主分片recovery时的最大并发数，默认为4。由于主分片的恢复不涉及在网络上复制数据，仅在本地磁盘读写，所以在节点配置了多个数据磁盘的情况下，可以适当提高此值。
- 在重启集群之前，先停止写入端，执行sync flush，让恢复过程有机会跳过phase1。
- 适当地多保留些 translog，配置项 index.translog.retention.size 默认最大保留512MB, index.translog.retention.age默认为不超过12小时。调整这两个配置可让恢复过程有机会跳过phase1。
- 合并 Lucene 分段，对于冷索引甚至不再更新的索引执行_forcemerge，较少的Lucene分段可以提升恢复效率，例如，减少对比，降低文件传输请求数量。

最后，可以考虑允许主副分片存在一定程度的不一致，修改 ES 恢复流程，少量的不一致则跳过phase1。

因为时序的关系会出现新老数据交替。如何实现主副分片一致呢？

答案是在写流程中做异常处理，通过版本号来过滤掉过期操作。写操作有三种类型：索引新文档、更新、删除。索引新文档不存在冲突问题，更新和删除操作采用相同的处理机制。每个操作都有一个版本号，这个版本号就是预期doc版本，它必须大于当前Lucene中的doc版本号，否则就放弃本次操作。对于更新操作来说，预期版本号是Lucene doc版本号+1。主分片节点写成功后新数据的版本号会放到写副本的请求中，这个请求中的版本号就是预期版本号。

## **gateway模块**

负责集群元信息的存储和集群重启时的恢复。

元数据信息是根据版本号选举出来的，而元数据写入成功的条件是“多数”，因此，保证进入recovery的条件为节点数量为“多数”，可以保证集群级和索引级的一致性。

获取各节点存储的元数据，然后根据版本号选举时，仅向具有Master资格的节点获取元数据。

## allocation模块

分片分配就是把一个分片指派到集群中某个节点的过程。分配决策由主节点完成，分配决策包含两方面：

- 哪些分片应该分配给哪些节点；
- 哪个分片作为主分片，哪些作为副分片。

对于新建索引和已有索引，分片分配过程也不尽相同。不过不管哪种场景，ES都通过两个基础组件完成工作：allocators和deciders。allocators尝试寻找最优的节点来分配分片，deciders则负责判断并决定是否要进行这次分配。

对于新建索引，allocators负责找出拥有分片数最少的节点列表，并按分片数量升序排序，因此分片较少的节点会被优先选择。所以对于新建索引，allocators的目标就是以更均衡的方式把新索引的分片分配到集群的节点中。然后deciders依次遍历allocators给出的节点，并判断是否把分片分配到该节点。

对于已有索引，则要区分主分片还是副分片。对于主分片，allocators只允许把主分片指定在已经拥有该分片完整数据的节点上。而对于副分片，allocators则是先判断其他节点上是否已有该分片的数据的副本（即便数据不是最新的）。如果有这样的节点，则allocators优先把分片分配到其中一个节点。因为副分片一旦分配，就需要从主分片中进行数据同步，所以当一个节点只拥分片中的部分数据时，也就意味着那些未拥有的数据必须从主节点中复制得到。这样可以明显地提高副分片的数据恢复速度。

触发分片分配有以下几种情况：

- index增删；
- node增删；
- 手工reroute；
- replica数量改变；
- 集群重启。

gateway阶段恢复的集群状态中，我们已经知道集群一共有多少个索引，每个索引的主副分片各有多少个，但是不知道它们位于哪个节点，现在需要找到它们都位于哪个节点。集群完全重启的初始状态，所有分片都被标记为未分配状态，此处也被称作分片分配过程。因此分片分配的概念不仅仅是分配一个全新分片。对于索引某个特定分片的分配过程中，先分配其主分片，后分配其副分片。

- 请求分片信息的fetchData请求效率低，可以借鉴HDFS的上报块状态流程。
- 不需要等所有主分片都分配完才执行副分片的分配。每个分片有自己的分配流程。
- 不需要等所有分片都分配完才执行recovery流程。
- 主分片不需要等副分片分配成功才进入主分片的recovery，主副分片有自己的recovery流程。

## Snapshot模块

快照不仅可以对索引备份，还可以将模板一起保存。恢复到的目标集群不需要相同的节点规模，只要它的存储空间足够容纳这些数据即可。

当仓库被删除时，ES只是删除快照的仓库位置引用信息，快照本身没有删除

快照操作在主分片上执行。快照执行期间，不影响集群正常的读写操作。在快照开始前，会执行一次flush，将操作系统内存“cache”的数据刷盘。因此通过快照可以获取从成功执行快照的时间点开始，磁盘中存储的Lucene数据，不包括后续的新增内容。

但是每次快照过程是增量的，下一次快照只会包含新增内容。可以在任何时候为集群创建一个快照过程，无论集群健康是Green、Yellow，还是Red。执行快照期间，被快照的分片不能移动到另一个节点，这可能会干扰重新平衡过程和分配过滤（allocation filtering）。这种分片迁移只可以在快照完成时进行。

快照开始后，可以用快照信息API和status API来监控进度。

在设计上，快照和恢复在同一个时间点只允许运行一个快照或一个恢复操作。如果想终止正在进行的快照操作，则可以使用删除快照命令来终止它。删除快照操作将检查当前快照是否正在运行，如果正在运行，则删除操作会先停止快照，然后从仓库中删除数据。如果是已完成的快照，则直接从仓库中删除快照数据。

恢复操作使用标准分片恢复机制。因此，如果要取消正在运行的恢复，则可以通过删除正在恢复的索引来实现。注意，索引数据将全部删除。

恢复过程是基于ES标准恢复机制的，因此标准的恢复监控服务可以用来监视恢复的状态。当执行集群恢复操作时通常会进入Red状态，这是因为恢复操作是从索引的主分片开始的，在此期间主分片状态变为不可用，因此集群状态表现为Red。一旦ES主分片恢复完成，整个集群的状态将被转换成Yellow，并且开始创建所需数量的副分片。一旦创建了所有必需的副分片，集群转换到Green状态。

ES的快照创建是基于Lucene快照实现的。但是Lucene中的快照概念与ES的并不相同。

Lucene快照是对最后一个提交点的快照，一次快照包含最后一次提交点的信息，以及全部分段文件。因此这个快照实际上就是对已刷盘数据的完整的快照。注意Lucene中没有增量快照的概念。每一次都是对整个Lucene索引完整快照，它代表这个Lucene索引的最新状态。之所以称为快照，是因为从创建一个Lucene快照开始，与此快照相关的物理文件都保证不会删除。在Lucene中，快照通过SnapshotDeletionPolicy实现。从Lucene 2.3版本开始支持。

Lucene快照负责获取最新的、已刷盘的分段文件列表，并保证这些文件不被删除，这个文件列表就是ES要执行复制的文件。

ES负责数据复制、仓库管理、增量备份，以及快照删除。

ES创建快照的过程涉及3种类型的节点：

- 协调节点，接收客户端请求，转发到主节点。
- 主节点，将创建快照相关的请求信息放到集群状态中广播下去，数据节点收到后执行数据复制。同时负责在仓库中写入集群状态数据。
- 数据节点，负责将Lucene文件复制到仓库，并在数据复制完毕后清理仓库中与任何快照都不相关的文件。由于数据分布在各个数据节点，因此复制操作必须由数据节点执行。每个数据节点将快照请求中本地存储的主分片复制到仓库。

快照过程是对Lucene物理文件的复制过程

Lucene索引可能由多个分段（segment）组成，每个分段是一个完全独立的索引，可以独立执行搜索。有两种情况产生新的分段：

- refresh操作产生一个Lucene分段。为新添加的documents创建新的分段。
- 已存在的分段合并，产生新分段。一次对Lucene索引的搜索需要搜索全部分段。

主节点的主要处理过程是将请求转换成内部需要的数据结构，提交一个集群任务进行处理，集群任务处理后生成的集群状态中会包含请求快照的信息，主节点将新生成的集群状态广播下去，数据节点收到后执行相应的实际数据的快照处理。

数据节点负责实际的快照实现，从全部将要快照的分片列表中找出存储于本节点的分片，对这些分片创建Lucene快照，复制文件。

由于ES的快照基于Lucene快照实现

Lucene的快照在SnapshotDeletionPolicy#snapshot方法中实现。该方法返回一个提交点，通过提交点可以获取分片的最新状态，包括全部Lucene 分段文件的列表。从得到这个列表开始，列表中的文件都不会被删除，直到释放提交点。

ES 快照的核心流程就是根据这个提交点创建快照

当前的Lucene提交点代表分片的最新状态，它包含全部Lucene分段。如果不考虑增量备份，则把这个文件列表全部复制到仓库就可以了。但是我们要实现的是每次快照都是增量的。实现方法就是计算出两个列表：

- 新增文件列表，代表将要复制到仓库的文件。遍历Lucene 提交点中的文件列表，如果仓库中已存在，则过滤掉，得到一个新增文件列表。
- 当前快照使用的全部文件列表，未来通过它找到某个快照相关的全部相关文件。这个列表的内容就是Lucene提交点中的文件列表的全部文件。

Lucene文件的“不变性”，除了write.lock、segments.gen两个文件，其他所有文件都不会更新，只写一次（write once）。锁文件 write.lock不需要复制。segments.gen是较早期的Lucene版本中存在的一种文件，我们不再讨论。因此，所有需要复制的文件都是不变的，无须考虑被更新的可能。所以在增量备份时，通过文件名就可以识别唯一的文件。但是在存储到仓库时，ES将文件全部重命名为以一个递增序号为名字的文件，并维护了对应关系。

ES删除快照的核心思想就是，在要删除的快照所引用的物理文件中，对不被任何其他快照使用的文件执行删除。每个快照都在自己的元信息文件（snap-*）中描述了本快照使用的文件列表。想要删除一些文件时，也不需要引用计数，只要待删除文件不被其他快照使用就可以安全删除。

快照删除过程/取消过程涉及3种类型的节点：

- 协调节点，接收客户端请求、转发到主节点。
- 主节点，将删除创建快照相关的请求信息放到集群状态中广播下去，删除快照和取消运行中的快照是同一个请求。数据节点负责取消运行中的快照创建任务，主节点负责删除已创建完毕的快照。无论如何，集群状态都会广播下去。当集群状态发布完毕，主节点开始执行删除操作。所以现在知道为什么主节点也要访问仓库了。删除操作确实没有必要要求各个数据节点去执行，任何节点都能看到仓库的全部数据，只需要单一节点执行删除即可，因此删除操作由主节点执行。
- 数据节点，负责取消正在运行的快照任务。

主节点将快照命令放到集群状态中广播下去，以此控制数据节点执行任务。数据节点执行完毕向主节点主动汇报状态。

- ES的配置文件更新后不能动态生效。但是提供了REST接口来调整需要动态更新的参数。path.repo 字段需要写到配置文件中。当需要迁移数据时就要先改配置重启集群，这样就不够方便。为什么不放在REST请求信息中，而要求配置到文件里？
- 集群永久设置、模板都保存在集群状态中，默认为不进行快照和恢复。注意索引别名不在集群状态中。快照默认会保存别名信息。
- Lucene段合并会导致增量快照时产生新增内容。当段文件比较小时，在HDFS中可能会产生许多小文件。因此通过force_merge API手工合并分段也有利于减少HDFS上的这些小文件。
- 快照写入了两个层面的元数据信息：集群层和索引层。
- 快照与集群是否健康无关，集群Red时也可以对部分索引执行快照。
- 数据复制过程中会计算校验和，确保复制后数据的正确性。
- 数据节点并发复制数据时取决于线程池的线程数的最大值，该值为 min（5,（处理器数量）/2）。
- 快照只对主分片执行。

## Cluster模块

发布集群状态是一个分布式事务操作，分布式事务需要实现原子性：要么所有参与者都提交事务，要么都取消事务。ES 使用二段提交来实现分布式事务。二段提交可以避免失败回滚，其基本过程是：把信息发下去，但不应用，如果得到多数节点的确认，则再发一个请求出去要求节点应用。

ES实现二段提交与标准二段提交有一些区别，发布集群状态到参与者的数量并非定义为全部，而是多数节点成功就算成功。多数的定义取决于配置项：

```go
discovery.zen.minimum_master_nodes
```

两个阶段过程如下。

- 发布阶段：发布集群状态，等待响应。
- 提交阶段：收到的响应数量大于minimum_master_nodes数量，发送commit请求。

二段提交不能保证第二阶段节点收到commit请求后正确应用事务。它只能保证参与者都提交了事务，不能保证事务在单个节点上提交成功还是失败，因此事务的执行可能在部分节点失败

二段提交发送的第一个请求是发布的集群状态数据。在发送第一个请求的响应处理中检查是否达到发送第二个请求的条件，触发提交阶段。

当条件满足后，正式发送提交请求

一旦进入提交阶段，发布过程就进入不可逆状态，如果有节点应用失败了，则整个发布过程不会被认为失败。即使只有少数节点正常应用集群状态，最终也只能接受这种结果。

如果从节点在应用集群状态时中止，例如，节点被“kill”，服务器断电等异常，则节点的集群状态应用失败。当节点重启之后，主动从Master节点获取最新的集群状态并应用。

## Transport模块

用于节点间内部通信

传输机制是完全异步的，这意味着没有阻塞线程等待响应。使用异步通信的好处是解决了C10k问题，也是广播请求/收集结果（例如，ES中的搜索）的理想解决方案。

默认情况下，ES的每个节点与其他节点都保持13个长连接，这在集群规模较大时，例如，达到1000节点时，会维护非常多的连接。在这种情况下，如果重新启动集群，由于需要在短时间内建立大量连接，则新建连接的请求有可能被操作系统认为是SYN攻击。

## 线程池

- 每种不同类型的线程池有各自不同的队列类型。scaling类型的线程池动态维护线程池数量。fixed_auto_queue_size与fix类型的线程池都有固定的线程数。
- ThreadPool类在节点启动时初始化，然后将类的引用传递给其他模块，其他模块通过明确指定线程名称从ThreadPool类中获取对应的线程池，然后执行自定义的任务。
- 节点关闭时，对线程池模块先调用shutdown，等待10秒后，执行shutdownNow。因此线程池中的任务有机会执行完毕，但在超时后会尝试终止线程池中的任务。

## Shrink原理

Shrink API缩小索引分片数。当索引缩小完成后，源索引可以删除。

新索引的主分片数必须是源索引主分片数的因数。例如，8个分片可以缩小到4、2、1个分片。

从源索引到目的索引创建硬链接。如果操作系统不支持硬链接，则复制Lucene分段。

使用硬链接，删除源索引，只是将文件的硬链接数量减1，删除源索引和目的索引中的任何一个，都不影响另一个正常读写。

## 写入速度优化

- 加大translog flush间隔，目的是降低iops、writeblock。· 加大index refresh间隔，除了降低I/O，更重要的是降低了segment merge频率。
- 调整bulk请求。
- 优化磁盘间的任务均匀情况，将shard尽量均匀分布到物理主机的各个磁盘。
- 优化节点间的任务分布，将任务尽量均匀地发到各节点。
- 优化Lucene层建立索引的过程，目的是降低CPU占用率及I/O，例如，禁用_all字段。

```go
index.translog.durability: async
//设置为async表示translog的刷盘策略按sync_interval配置指定的时间周期进行。

index.translog.sync_interval: 120s
//加大translog刷盘间隔时间。默认为5s，不可低于100ms。

index.translog.flush_threshold_size: 1024mb
//超过这个大小会导致refresh操作，产生新的Lucene分段。默认值为512MB。

//索引刷新间隔refresh_interval
index.refresh_interval: 120s
```

indexing buffer在为doc建立索引时使用，当缓冲满时会刷入磁盘，生成一个新的segment，这是除refresh_interval刷新索引外，另一个生成新segment的机会。每个shard有自己的indexing buffer，下面的这个buffer大小的配置需要除以这个节点上所有shard的数量

```go
indices.memory.index_buffer_size默认为整个堆空间的10%。
indices.memory.min_index_buffer_size默认为48MB。
indices.memory.max_index_buffer_size默认为无限制。
```

使用bulk请求

- 批量写比一个索引请求只写单个文档的效率高得多，但是要注意bulk请求的整体字节数不要太大，太大的请求可能会给集群带来内存压力，因此每个请求最好避免超过几十兆字节，即使较大的请求看上去执行得更好。

自动生成doc ID

- 通过ES写入流程可以看出，写入doc时如果外部指定了id，则ES会先尝试读取原来doc的版本号，以判断是否需要更新。这会涉及一次读取磁盘的操作，通过自动生成doc ID可以避免这个环节。

调整字段Mappings

调整_source字段

禁用_all字段

## 搜索速度的优化

为文件系统cache预留足够的内存

应该至少为系统cache预留一半的可用物理内存，更大的内存有更高的cache命中率。

为了让搜索时的成本更低，文档应该合理建模。特别是应该避免join操作，嵌套（nested）会使查询慢几倍，父子（parent-child）关系可能使查询慢数百倍

预索引数据还可以针对某些查询的模式来优化数据的索引方式。

有些字段的内容是数值，但并不意味着其总是应该被映射为数值类型，例如，一些标识符，将它们映射为keyword可能会比integer或long更好。

应该避免使用脚本。如果一定要用，则应该优先考虑painless和expressions。

优化日期搜索在使用日期范围检索时，使用now的查询通常不能缓存，因为匹配到的范围一直在变化。

为只读索引执行force-merge

预热全局序号（global ordinals）

- 全局序号是一种数据结构，用于在keyword字段上运行terms聚合。它用一个数值来代表字段中的字符串值，然后为每一数值分配一个 bucket。这需要一个对global ordinals 和 bucket的构建过程。默认情况下，它们被延迟构建，因为ES不知道哪些字段将用于 terms聚合，哪些字段不会。可以通过配置映射在刷新（refresh）时告诉ES预先加载全局序数

预热文件系统cache

- 如果ES主机重启，则文件系统缓存将为空，此时搜索会比较慢。可以使用index.store.preload设置，通过指定文件扩展名，显式地告诉操作系统应该将哪些文件加载到内存中
- 如果文件系统缓存不够大，则无法保存所有数据，那么为太多文件预加载数据到文件系统缓存中会使搜索速度变慢，应谨慎使用。

限制搜索请求的分片数

- 一个搜索请求涉及的分片数量越多，协调节点的CPU和内存压力就越大。

利用自适应副本选择（ARS）提升ES响应速度（ES 7.0开始，ARS将默认开启）降低p99指标

## 磁盘优化

禁用_source

需要访问_source的API将无法使用，至少包括下列情况：· update、update_by_query、reindex；· 高亮搜索；· 重建索引（包括更新mapping、分词器，或者集群跨大版本升级可能会用到）；· 调试聚合查询功能，需要对比原始数据。

Fource Merge一个ES索引由若干分片组成，一个分片有若干Lucene分段，较大的Lucene分段可以更有效地存储数据。使用_forcemerge API来对分段执行合并操作，通常，我们将分段合并为一个单个的分段：max_num_segments=1。

在文档中以相同的顺序放置字段

## 问题

如何扩展

数据如何被划分为分片和复制的

索引和搜索时如何进行的

集群中多个节点是如何工作的

主副分片如何同步